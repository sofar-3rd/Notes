# VMware-FT

## 复制和容错

复制能解决的问题：

* **fail-stop 故障**：计算机因为电源、过热等故障停止运行；
* 一些能转换为 **fail-stop 故障**的硬件、软件 bug，例如软件错误导致了服务器崩溃，磁盘上的数据出错被检测出；

复制不能解决的问题：

* **软件bug**，当 master 节点的软件出现了错误，其他副本会接收相同的错误结果。

## 状态转移和复制状态机

* **状态转移**：primary 服务器每隔一段时间后将内存快照发送给 back-up 节点，back-up 节点将内存快照写入内存进行全量复制；
* **复制状态机**：primary 节点和 back-up 节点保持相同的初始状态，并同时接收命令，并在机器上执行相同的操作，保持相同的状态；
  * 需要考虑主从同步频率；
  * 处理随机化操作，主或从节点执行命令失败（可能导致主从状态不一致）；

## VMware-FT工作原理

VM-FT结构图：

![alt text](/MIT6824/VM-FT/Configuration.png){:height="60%" width="60%"}

结构介绍：

* primary 和 backup 各自运行在不同的服务器的VMM上;
* primary, backup 以及 client 都运行在同一个局域网;
* VMware FT里的多副本服务没有使用本地盘，而是使用了一些Disk Server（远程盘）;
* primary 和 backup 之间通信的通道称为Log Channel.

VM-FT工作流程:

![alt text](/MIT6824/VM-FT/workprocess.png)

命令执行流程：

1. client 发送包含命令的数据包；
2. 数据包服务器上被收到, 产生一个中断被 primary 的 VMM 捕获；
3. primary的VMM捕获到中断后, 进行如下两项操作：
   1. 在自己虚拟化的 OS 中, 模拟网络数据包到达的中断；
   2. 将网络数据包拷贝一份，并通过 `Log Channel` 送给 Backup 虚机所在的 VMM；
4. Backup 虚机所在的 VMM 收到数据包, 同样在自己虚拟化的 OS 中, 模拟网络数据包到达的中断；
5. primary 和 backup 执行数据包中的命令；
6. primary 产生回复报文, 通过 VMM 返回给客户端；
7. Backup 的 VMM 将产生的回复丢弃。

## 非确定性事件

非确定性事件分为以下几类：

* **客户端输入**。一个来源于客户端的网络数据包由于其随时到达呈现出不可预期性。当网络数据包送达时，通常网卡的 DMA（Direct Memory Access）会将网络数据包的内容拷贝到内存，之后触发一个中断。操作系统会在处理指令的过程中消费这个中断。**对于Primary和Backup，最好要在相同的时间，相同的位置触发，否则执行过程就是不一样的，进而会导致它们的状态产生偏差。**
  * 例子：当用户向博客中添加了一篇文章，并统计当前博客的文章总量并写入个人简介中，必须保证主从节点对于上述两条命令的执行顺序保持一致，不能因为数据包到达的不可预测性而导致主从节点在不同的状态下执行命令导致状态偏差。
* **怪异指令**
  * 随机数生成器；
  * 获取当前时间的指令；
  * 获取计算机的唯一ID。
  * ......
* **多CPU的并发**。在两台的 back-up 服务器分别由不同的线程执行任务所导致的结果可能不同，所以多核是一个巨大的非确定性事件来源。

Primary 服务器和 back-up 服务器之间通过 Log Channel 来同步，日志应该包含以下三种数据：

1. **事件发生时的指令序号**。通过指令序号不同服务器可以同步执行命令的时机；
2. **日志条目的类型**。可能是网络数据输入，也可能是怪异指令；
3. **数据**。如果是一个网络数据包，那么数据就是网络数据包的内容。如果是一个怪异指令，数据将会是这些怪异指令在Primary上执行的结果。这样Backup虚机就可以伪造指令，并提供与Primary相同的结果。

### 如何处理网络数据包到达

在通常情况下，当网络数据包到达时，网卡会通过 DMA 的方式直接将数据包拷贝到计算机的内存中。但在 VM-FT 中，物理服务器的网卡会将网络数据包拷贝给 VMM 的内存，之后，网卡中断会送给 VMM 。这时，**VMM 会暂停 Primary 虚机，记住当前的指令序号**，将整个网络数据包拷贝给 Primary 虚机的内存，之后模拟一个网卡中断发送给primary虚机。primary 将网络数据包拷贝给Backup虚机，**之后在相同的指令序号位置模拟一个网卡中断发送给 Backup 虚机**。这就是论文中介绍的Bounce Buffer机制。

## 输出控制

通过确定性重放操作，副本 VM 能够从 logging chanel 中读取主 VM 的各种操作，并实时在副本 VM 中执行类似操作。为了实现日志项的容错能力，本节定义了容错协议，主要是 **Output Requirement** 和 **Output Rule**。

**Output Requirement**：**如果主 VM 发生故障后副本 VM 接管后，副本 VM 必须接管原来主 VM 已经完成或部分完成的工作，对外界的输出要保持一致**。 即 failover 期间，上述操作对客户端是透明的，客户端不会被中断服务或者是察觉到不一致。

会出现这样一种情况，当用户对储存的数据进行修改，在 primary 服务器对数据进行修改后并向客户端修改成功后。primary 服务器崩溃，并且没有成功将修改数据的指令传递给 back-up 服务器。此时当 back-up 服务器上任替换 primary 服务器后，用户再次查询数据时会得到原来的旧数据。

**Output Rule**：**如果主 VM 欲产生到客户端的输出，其必须延迟这项输出操作，直到主 VM 收到来自于副本 VM 的关于该输出的确认日志项**。

![alt text](/MIT6824/VM-FT/delayOuput.png)

## 重复输出

回复报文已经从 VMM 发往客户端了，所以客户端收到了回复，但是这时 Primary 虚机崩溃了。**而在 Backup 侧，客户端请求还堆积在 Backup 对应的 VMM 的 Log 等待缓冲区，也就是说客户端请求还没有真正发送到 Backup 虚机中**。

当 Primary 崩溃之后，Backup 接管服务，Backup 首先需要消费所有在等待缓冲区中的 Log，以保持与 Primay 在相同的状态，这样 Backup 才能以与 Primary 相同的状态接管服务。而当 Backup 处理缓冲区的客户端请求时，会产生重复的输出至客户端，但**客户端可以通过 TCP 协议相同序列号判断数据包是否重复从而决定是否丢弃数据包**，这个问题在传输层解决了。

对于任何有主从切换的复制系统，基本上不可能将系统设计成不产生重复输出。为了避免重复输出，有一个选项是在两边都不生成输出，但这是一个非常糟糕的做法（**因为对于客户端来说就是一次失败的请求**）。当出现主从切换时，切换的两边都有可能生成重复的输出，这意味着所有复制系统的客户端需要一种重复检测机制。**上述使用的是TCP来完成重复检测，如果没有TCP，那或许就需要应用程序级别的序列号**。

## Test and Set

**test and set 主要用于解决脑裂问题，如果副本 VM 停止收到心跳、或者发生网络分区后，主 VM 和副本 VM 都认为对方发生故障，此时 主 VM 和副本 VM 都上线为客户端提供服务，造成脑裂问题**。

为解决该问题，必须要确保发生以上故障时，主 VM 和副本 VM 仅能有一个上线提供服务，本论文使用了虚拟磁盘的共享存储来解决问题，在共享存储上执行 atomic test-and-set 操作：

* 如果成功，则该 VM 上线服务；
* 如果失败，表示已经有 VM 上线服务了，因此当前 VM 不会上线；
* 如果当前操作的 VM 无法访问共享存储，则其一直重试。

如果共享存储无法访问，由于主 VM 和副本 VM 都共享该存储，则意味着此时 VM 也无法工作。因此，使用共享存储解决脑裂问题不会引入额外的不可用。
